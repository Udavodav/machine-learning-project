import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.metrics import (roc_auc_score, f1_score, precision_score, recall_score, 
                           accuracy_score, confusion_matrix, roc_curve, average_precision_score)
from sklearn.calibration import CalibratedClassifierCV
import optuna
from optuna.samplers import TPESampler
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.append('..')
from notebooks.pckgs.analyze import *


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
RANDOM_STATE = 42
N_FOLDS = 5
SCORING = 'roc_auc'

optuna.logging.set_verbosity(optuna.logging.WARNING) 

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò ===========
def randForestOpt(trial, X_train, y_train):
    """—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ RandomForest"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 5, 15),
        'min_samples_split': trial.suggest_int('min_samples_split', 5, 30),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 20),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
        'bootstrap': True,
        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),
    }
    
    model = RandomForestClassifier(**params, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1)
    
    cv_scores = cross_val_score(
        model, 
        X_train,
        y_train,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),
        scoring='roc_auc',
        n_jobs=-1
    )
    
    return cv_scores.mean()

def gradOpt(trial, X_train, y_train):
    """—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GradientBoosting"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        'min_samples_split': trial.suggest_int('min_samples_split', 10, 50),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 4, 20),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
        'subsample': trial.suggest_float('subsample', 0.5, 0.9),
    }
    
    model = GradientBoostingClassifier(**params, random_state=RANDOM_STATE)
    
    cv_scores = cross_val_score(
        model, 
        X_train,
        y_train,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),
        scoring='roc_auc',
        n_jobs=-1
    )
    
    return cv_scores.mean()

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–• ===========
def load_data(type_data='balanced', train_path=None, test_path=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø—É—Ç–∏ –∏–ª–∏ –ø—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if train_path is None:
        train_path = f"../data/processed/train_{type_data}.csv"
    if test_path is None:
        test_path = f"../data/processed/test.csv"
    
    X_train_bal = pd.read_csv(train_path).drop('Churn', axis=1)
    y_train_bal = pd.read_csv(train_path)['Churn']
    X_test_bal = pd.read_csv(test_path).drop('Churn', axis=1)
    y_test_bal = pd.read_csv(test_path)['Churn']
    
    return X_train_bal, y_train_bal, X_test_bal, y_test_bal

def split_data_for_calibration(X_train_bal, y_train_bal, test_size=0.2, random_state=42):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏"""
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_bal, y_train_bal, 
        test_size=test_size, 
        stratify=y_train_bal,
        random_state=random_state
    )
    return X_train, X_val, y_train, y_val

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ò –û–¶–ï–ù–ö–ò ===========
def optimize_hyperparameters(X_train, y_train, optimize_func, n_trials=50):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π"""
    print("–ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    study_rf = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))
    study_rf.optimize(lambda trial: optimize_func(trial, X_train, y_train), 
                      n_trials=n_trials, show_progress_bar=True)
    
    return study_rf.best_params

def train_and_evaluate_models(models_dict, X_train, y_train, X_test, y_test, cv_strategy=None, scoring='roc_auc'):
    """–û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    if cv_strategy is None:
        cv_strategy = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    
    results = []
    
    for model_name, model in models_dict.items():
        print(f"\n{model_name.upper()}")
        print("-" * 40)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        print("–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (5-fold stratified)")
        cv_scores = cross_val_score(
            model, X_train, y_train,
            cv=cv_strategy,
            scoring=scoring,
            n_jobs=-1
        )
        print(f"   ROC-AUC –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ: {cv_scores}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π ROC-AUC: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        metrics = calculate_metrics(model_name, y_test, y_pred, y_proba, cv_scores)
        results.append(metrics)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        postfix = model_name.lower().replace(' ', '_')
        rocPlot(y_test, y_proba, f"{type_data}_{postfix}")
        erorrMatrixPlot(y_test, y_proba, f"{type_data}_{postfix}")
        
        print_metrics(metrics)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ
        models_dict[model_name] = model
    
    return pd.DataFrame(results), models_dict

def calculate_metrics(model_name, y_true, y_pred, y_proba, cv_scores=None):
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏"""
    metrics = {
        'model': model_name,
        'test_roc_auc': roc_auc_score(y_true, y_proba),
        'test_pr_auc': average_precision_score(y_true, y_proba),
        'test_f1': f1_score(y_true, y_pred),
        'test_precision': precision_score(y_true, y_pred),
        'test_recall': recall_score(y_true, y_pred),
        'test_accuracy': accuracy_score(y_true, y_pred)
    }
    
    if cv_scores is not None:
        metrics['cv_roc_auc_mean'] = cv_scores.mean()
        metrics['cv_roc_auc_std'] = cv_scores.std()
    
    return metrics

def print_metrics(metrics):
    """–ü–µ—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏"""
    print(f"–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:")
    print(f"   ROC-AUC:     {metrics['test_roc_auc']:.3f}")
    print(f"   PR-AUC:      {metrics['test_pr_auc']:.3f}")
    print(f"   F1-Score:    {metrics['test_f1']:.3f}")
    print(f"   Precision:   {metrics['test_precision']:.3f}")
    print(f"   Recall:      {metrics['test_recall']:.3f}")
    print(f"   Accuracy:    {metrics['test_accuracy']:.3f}")

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ö–ê–õ–ò–ë–†–û–í–ö–ò ===========
def calibrate_model(base_model, X_train, y_train, X_val, y_val, X_test, y_test):
    """–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    # –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
    base_model.fit(X_train, y_train)
    
    # Platt scaling (SIGMOID CALIBRATION)
    calibrated_model = CalibratedClassifierCV(
        base_model,
        method='sigmoid',
        cv='prefit'
    )
    calibrated_model.fit(X_val, y_val)
    
    # –ò–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
    isotonic_model = CalibratedClassifierCV(
        base_model,
        method='isotonic',
        cv='prefit'
    )
    isotonic_model.fit(X_val, y_val)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    print("\n–û—Ü–µ–Ω–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    y_proba_base = base_model.predict_proba(X_test)[:, 1]
    y_proba_sigmoid = calibrated_model.predict_proba(X_test)[:, 1]
    y_proba_isotonic = isotonic_model.predict_proba(X_test)[:, 1]
    
    calibration_results = pd.DataFrame({
        'Model': ['–ë–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏', 'Platt scaling', 'Isotonic regression'],
        'ROC-AUC': [
            roc_auc_score(y_test, y_proba_base),
            roc_auc_score(y_test, y_proba_sigmoid),
            roc_auc_score(y_test, y_proba_isotonic)
        ],
        'PR-AUC': [
            average_precision_score(y_test, y_proba_base),
            average_precision_score(y_test, y_proba_sigmoid),
            average_precision_score(y_test, y_proba_isotonic)
        ],
        'Brier Score': [
            np.mean((y_proba_base - y_test) ** 2),
            np.mean((y_proba_sigmoid - y_test) ** 2),
            np.mean((y_proba_isotonic - y_test) ** 2)
        ]
    })
    
    print(calibration_results.to_string(index=False))
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
    best_calibrated_idx = calibration_results['ROC-AUC'].idxmax()
    best_calibrated_model_name = calibration_results.loc[best_calibrated_idx, 'Model']
    
    if best_calibrated_model_name == 'Platt scaling':
        final_model = calibrated_model
    elif best_calibrated_model_name == 'Isotonic regression':
        final_model = isotonic_model
    else:
        final_model = base_model
    
    return final_model, best_calibrated_model_name, calibration_results

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ê–ù–°–ê–ú–ë–õ–ï–ô ===========
def create_voting_ensemble(estimators, voting='soft', weights=None):
    """–°–æ–∑–¥–∞–Ω–∏–µ Voting –∞–Ω—Å–∞–º–±–ª—è"""
    return VotingClassifier(
        estimators=estimators,
        voting=voting,
        weights=weights,
        n_jobs=-1
    )

def create_stacking_ensemble(estimators, final_estimator, cv=3):
    """–°–æ–∑–¥–∞–Ω–∏–µ Stacking –∞–Ω—Å–∞–º–±–ª—è"""
    return StackingClassifier(
        estimators=estimators,
        final_estimator=final_estimator,
        cv=cv,
        passthrough=False,
        n_jobs=-1
    )

def evaluate_ensemble(ensemble, X_train, y_train, X_test, y_test, ensemble_name):
    """–û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è"""
    print(f"–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è {ensemble_name}...")
    ensemble.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    ensemble_proba = ensemble.predict_proba(X_test)[:, 1]
    ensemble_pred = ensemble.predict(X_test)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    rocPlot(y_test, ensemble_proba, f"{ensemble_name}_{type_data}")
    erorrMatrixPlot(y_test, ensemble_proba, f"{ensemble_name}_{type_data}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    metrics = calculate_metrics(
        ensemble_name, 
        y_test, 
        ensemble_pred, 
        ensemble_proba
    )
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è {ensemble_name}:")
    print_metrics(metrics)
    
    return ensemble, metrics

# =========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø ===========
def save_model(model, filepath):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    joblib.dump(model, filepath)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")

def save_results(results_df, filepath):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    results_df.to_csv(filepath, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")

# =========== –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù ===========
def run_pipeline(type_data='balanced_medium'):
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"""
    print(f"–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö: {type_data}")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train_bal, y_train_bal, X_test, y_test = load_data(type_data)
    
    print(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –û–±—É—á–∞—é—â–∞—è (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è): {X_train_bal.shape}")
    print(f"  –¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    X_train, X_val, y_train, y_val = split_data_for_calibration(X_train_bal, y_train_bal)
    
    print(f"  Train –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {X_train.shape}")
    print(f"  Val –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {X_val.shape}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    forest_params = optimize_hyperparameters(X_train_bal, y_train_bal, randForestOpt)
    grad_params = optimize_hyperparameters(X_train_bal, y_train_bal, gradOpt)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models = {
        'Random Forest': RandomForestClassifier(
            **forest_params,
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),
        'Gradient Boosting': GradientBoostingClassifier(**grad_params, random_state=RANDOM_STATE)
    }
    
    # –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    results_df, trained_models = train_and_evaluate_models(
        models, X_train, y_train, X_test, y_test
    )
    
    # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_info = results_df.loc[results_df['test_roc_auc'].idxmax()]
    best_model_name = best_model_info['model']
    
    print(f"\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ ROC-AUC: {best_model_name}")
    print(f"   ROC-AUC –Ω–∞ —Ç–µ—Å—Ç–µ: {best_model_info['test_roc_auc']:.3f}")
    print(f"   ROC-AUC –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_model_info['cv_roc_auc_mean']:.3f}")
    
    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    print(f"\n–ö–ê–õ–ò–ë–†–û–í–ö–ê –ú–û–î–ï–õ–ò {best_model_name}")
    
    if best_model_name == 'Random Forest':
        base_model = RandomForestClassifier(
            **forest_params,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
    else:
        base_model = GradientBoostingClassifier(
            **grad_params,
            random_state=RANDOM_STATE
        )
    
    final_model, best_calibration_method, calibration_results = calibrate_model(
        base_model, X_train, y_train, X_val, y_val, X_test, y_test
    )
    
    print(f"\n–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {best_model_name} —Å {best_calibration_method}")
    print(f"   ROC-AUC: {calibration_results.loc[calibration_results['Model'] == best_calibration_method, 'ROC-AUC'].values[0]:.3f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    y_proba_final = final_model.predict_proba(X_test)[:, 1]
    y_pred_final = (y_proba_final > 0.5).astype(int)
    postfix = best_model_name.lower().replace(' ', '_')
    rocPlot(y_test, y_proba_final, f"{postfix}_{type_data}")
    erorrMatrixPlot(y_test, y_proba_final, f"{postfix}_{type_data}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–π
    print("\n–°–û–ó–î–ê–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–ï–ô")
    print("=" * 30)
    
    # Voting –∞–Ω—Å–∞–º–±–ª—å
    voting_ensemble = create_voting_ensemble([
        ('rf', RandomForestClassifier(**forest_params, random_state=RANDOM_STATE, n_jobs=-1)),
        ('gb', GradientBoostingClassifier(**grad_params, random_state=RANDOM_STATE))
    ])
    
    voting_model, voting_metrics = evaluate_ensemble(
        voting_ensemble, X_train, y_train, X_test, y_test, 'ensemble_forest_boosting'
    )
    
    # Stacking –∞–Ω—Å–∞–º–±–ª—å
    stacking_ensemble = create_stacking_ensemble([
        ('lr', LogisticRegression(max_iter=100, random_state=RANDOM_STATE)),
        ('rf', RandomForestClassifier(**forest_params, random_state=RANDOM_STATE, n_jobs=-1))
    ], LogisticRegression(max_iter=100, random_state=RANDOM_STATE))
    
    stacking_model, stacking_metrics = evaluate_ensemble(
        stacking_ensemble, X_train, y_train, X_test, y_test, 'ensemble_forest_logistic'
    )
    
    # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    result_models = pd.DataFrame([
        {
            'model': 'final model',
            'ROC-AUC': roc_auc_score(y_test, y_proba_final),
            'PR-AUC': average_precision_score(y_test, y_proba_final),
            'Accuracy': accuracy_score(y_test, y_pred_final),
            'F1-Score': f1_score(y_test, y_pred_final),
            'Precision': precision_score(y_test, y_pred_final),
            'Recall': recall_score(y_test, y_pred_final)
        },
        {
            'model': 'ensemble forest+boosting',
            **voting_metrics
        },
        {
            'model': 'ensemble forest+logistic',
            **stacking_metrics
        }
    ])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
    save_model(final_model, f"../models/final_model_{type_data}.pkl")
    save_model(voting_model, f"../models/ensemble_forest_boosting_{type_data}.pkl")
    save_model(stacking_model, f"../models/ensemble_forest_logistic_{type_data}.pkl")
   
    #  –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"""
–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:
-----------------------------------------------------------
–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} —Å {best_calibration_method}
ROC-AUC: {roc_auc_score(y_test, y_proba_final):.3f} 
F1-Score: {f1_score(y_test, y_pred_final):.3f}
Precision: {precision_score(y_test, y_pred_final):.3f} (—Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç—Ç–æ–∫–∞)
Recall: {recall_score(y_test, y_pred_final):.3f} (–Ω–∞—Ö–æ–¥–∏–º {recall_score(y_test, y_pred_final):.1%} —É—à–µ–¥—à–∏—Ö)
""")
    
    return {
        'results_df': results_df,
        'final_model': final_model,
        'voting_model': voting_model,
        'stacking_model': stacking_model,
        'result_models': result_models
    }

# =========== –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê ===========
if __name__ == "__main__":
    # –ú–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    data_types = ['balanced_medium', 'balanced', 'unbalanced']
    
    for type_data in data_types:
        try:
            print(f"\n{'='*60}")
            print(f"–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•: {type_data.upper()}")
            print('='*60)
            results = run_pipeline(type_data)
            print(f"\n–ó–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {type_data}")
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {type_data}: {e}")



